{
  "title": "Automated Behavioral Persona Discovery from Social Media",
  "subtitle": "Building a production ML pipeline for audience segmentation using UMAP and HDBSCAN",
  "slug": "automated-persona-discovery-ml-pipeline",
  "excerpt": "A technical walkthrough of building an end-to-end machine learning system that automatically discovers 11 distinct behavioral personas from 1,618 tweets using Sentence-BERT embeddings, UMAP dimensionality reduction, and HDBSCAN clustering.",
  "content": "<h2>The Challenge</h2>\n<p>Traditional audience segmentation relies on manual analysis or predetermined demographic categories. This approach doesn't scale and often misses unexpected behavioral patterns in the data. We built a system to automatically discover behavioral personas from social media conversations about e-commerce.</p>\n\n<h2>System Overview</h2>\n<p>Starting with 1,618 raw tweets, our pipeline applies aggressive quality filtering, multi-modal feature extraction, dimensionality reduction, and density-based clustering to discover distinct behavioral segments.</p>\n\n<h3>Pipeline Architecture</h3>\n<pre><code>Raw Data (1,618 tweets)\n    ↓\nData Quality (6-stage filtering)\n    ↓\nClean Data (1,454 tweets, 89.9% retention)\n    ↓\nFeature Extraction\n    ├─ Text Embeddings (384-dim Sentence-BERT)\n    └─ Behavioral Features (36-dim)\n    ↓\nFeature Fusion (75% text + 25% behavioral = 420-dim)\n    ↓\nUMAP Reduction (420-dim → 50-dim)\n    ↓\nHDBSCAN Clustering\n    ↓\n11 Personas + Noise Handling\n    ↓\nEnrichment & API Layer</code></pre>\n\n<h2>Data Quality Pipeline</h2>\n<p>We applied 6-stage filtering to ensure high signal-to-noise ratio:</p>\n\n<pre><code>Stage 1: Exact Duplicates       1,618 → 1,515 (-6.4%)\nStage 2: Near Duplicates (>0.95) 1,515 → 1,480 (-2.3%)\nStage 3: Pure Retweets           1,480 → 1,480 (0%)\nStage 4: Short Tweets (<20 char) 1,480 → 1,454 (-1.8%)\nStage 5: Promotional Templates   1,454 → 1,454 (0%)\nStage 6: High-Freq Templates     1,454 → 1,454 (0%)\n──────────────────────────────────────────────\nFinal: 1,454 tweets (89.9% retention)</code></pre>\n\n<h2>Feature Engineering</h2>\n\n<h3>Text Embeddings</h3>\n<p>We used Sentence-BERT (all-MiniLM-L6-v2) to generate 384-dimensional semantic embeddings optimized for similarity tasks:</p>\n\n<pre><code class=\"language-python\">from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(tweets)\n# Output: (1454, 384)</code></pre>\n\n<h3>Behavioral Features (36 dimensions)</h3>\n<p>We extracted behavioral signals across four categories:</p>\n\n<ul>\n<li><strong>Temporal (8-dim):</strong> Hour of day, day of week, weekend flag, time patterns</li>\n<li><strong>Interaction (12-dim):</strong> Reply count, mention density, thread depth, conversation patterns</li>\n<li><strong>Style (10-dim):</strong> Text length, word count, punctuation density, URL/hashtag presence</li>\n<li><strong>Network (6-dim):</strong> Author consistency, account age proxies, posting frequency</li>\n</ul>\n\n<h3>Feature Fusion</h3>\n<p>Through ablation testing, we found that weighting text embeddings more heavily than behavioral features produces optimal results:</p>\n\n<pre><code class=\"language-python\">fused_features = 0.75 * text_embeddings + 0.25 * behavioral_features\n# Output: (1454, 420)</code></pre>\n\n<p>Performance comparison:</p>\n\n<table>\n<thead>\n<tr><th>Configuration</th><th>Coherence</th><th>ARI</th><th>Notes</th></tr>\n</thead>\n<tbody>\n<tr><td>Text Only (100%)</td><td>0.73</td><td>0.68</td><td>Misses behavioral nuance</td></tr>\n<tr><td><strong>Text (75%) + Behavioral (25%)</strong></td><td><strong>0.80</strong></td><td><strong>0.75</strong></td><td><strong>Best balance</strong></td></tr>\n<tr><td>Text (50%) + Behavioral (50%)</td><td>0.76</td><td>0.71</td><td>Over-weights behavior</td></tr>\n<tr><td>Behavioral Only (100%)</td><td>0.51</td><td>0.43</td><td>Insufficient signal</td></tr>\n</tbody>\n</table>\n\n<h2>Dimensionality Reduction with UMAP</h2>\n<p>We used UMAP to reduce from 420 to 50 dimensions while preserving both local and global structure:</p>\n\n<pre><code class=\"language-python\">import umap\n\nreducer = umap.UMAP(\n    n_neighbors=40,      # Preserve local + global structure\n    min_dist=0.4,        # Moderate compactness\n    n_components=50,     # Sufficient for HDBSCAN\n    random_state=42      # Reproducibility\n)\n\nreduced_features = reducer.fit_transform(fused_features)\n# Output: (1454, 50)</code></pre>\n\n<p><strong>Why UMAP over t-SNE:</strong> Better preservation of global structure, faster on medium datasets, and theoretical guarantees on topology preservation.</p>\n\n<h2>Clustering with HDBSCAN</h2>\n<p>HDBSCAN discovers clusters without requiring a predefined cluster count:</p>\n\n<pre><code class=\"language-python\">import hdbscan\n\nclusterer = hdbscan.HDBSCAN(\n    min_cluster_size=75,           # Minimum viable persona size\n    min_samples=10,                # Core point threshold\n    metric='euclidean',\n    cluster_selection_method='eom' # Excess of Mass\n)\n\nlabels = clusterer.fit_predict(reduced_features)</code></pre>\n\n<p><strong>Results:</strong></p>\n<ul>\n<li>11 distinct clusters discovered</li>\n<li>545 noise points (37.5%)</li>\n<li>Cluster sizes: 38-337 tweets</li>\n</ul>\n\n<h2>Noise Handling for 100% Coverage</h2>\n<p>Rather than discarding 37.5% of data labeled as noise, we assigned each noise point to its nearest cluster centroid:</p>\n\n<pre><code class=\"language-python\">from scipy.spatial.distance import euclidean\n\n# Compute cluster centroids\ncentroids = {}\nfor cluster_id in range(11):\n    mask = labels == cluster_id\n    centroids[cluster_id] = reduced_features[mask].mean(axis=0)\n\n# Assign noise points\nnoise_mask = labels == -1\nfor idx in np.where(noise_mask)[0]:\n    distances = {\n        cid: euclidean(reduced_features[idx], centroid)\n        for cid, centroid in centroids.items()\n    }\n    labels[idx] = min(distances, key=distances.get)</code></pre>\n\n<p>This achieves <strong>100% coverage</strong> while maintaining cluster quality.</p>\n\n<h2>Results: 11 Discovered Personas</h2>\n\n<h3>Quality Metrics</h3>\n<table>\n<thead>\n<tr><th>Metric</th><th>Value</th><th>Interpretation</th></tr>\n</thead>\n<tbody>\n<tr><td>Coherence Score</td><td>0.80</td><td>Excellent intra-cluster similarity (>0.70 is good)</td></tr>\n<tr><td>Adjusted Rand Index</td><td>0.75</td><td>Very strong cluster separation (>0.60 is good)</td></tr>\n<tr><td>Silhouette Score</td><td>0.45</td><td>Good cluster definition (>0.40 is good)</td></tr>\n<tr><td>Coverage</td><td>100%</td><td>All data points assigned</td></tr>\n</tbody>\n</table>\n\n<h3>Top 3 Personas</h3>\n\n<p><strong>1. Tech Builders (337 tweets, 23.2%)</strong></p>\n<ul>\n<li>Top terms: return, policy, new, days, items</li>\n<li>Peak activity: 3 PM</li>\n<li>Avg word count: 30.8 (concise, technical)</li>\n<li>Focus: Payment infrastructure, return policies, technical discussions</li>\n</ul>\n\n<p><strong>2. India Watchers (213 tweets, 14.7%)</strong></p>\n<ul>\n<li>Top terms: days, delivery, order, orders, product</li>\n<li>Peak activity: 3 PM</li>\n<li>Avg word count: 39.0</li>\n<li>Focus: India e-commerce market dynamics, quick commerce</li>\n</ul>\n\n<p><strong>3. Conversational Commerce Users (194 tweets, 13.3%)</strong></p>\n<ul>\n<li>Top terms: shopping, online, experience, ai, like</li>\n<li>Peak activity: 3 PM</li>\n<li>Avg word count: 26.4</li>\n<li><strong>Emerging trend:</strong> Largest AI-focused segment</li>\n</ul>\n\n<h2>Production API</h2>\n<p>We built a query API with 18 methods for real-world applications:</p>\n\n<pre><code class=\"language-python\">from ri_index import RIIndex\n\nri = RIIndex()\n\n# Basic queries\npersona = ri.get_persona(0)\npersonas = ri.list_personas(brief=True)\n\n# Filtering for marketing campaigns\ntargets = ri.filter_personas(\n    min_size=100,\n    peak_hour_range=(9, 17)\n)\n# Returns: 6 personas, 1,144 tweets (78.7% coverage)\n\n# Pain point identification\npain_personas = ri.filter_personas(\n    top_terms=['service', 'trust', 'fake', 'issues']\n)\n# Returns: 4 personas, 337 tweets (23.2% of conversations)\n\n# Similarity analysis\nsimilar = ri.find_similar_personas(\n    persona_id=0,\n    top_k=3,\n    method='language'\n)</code></pre>\n\n<h2>Business Applications</h2>\n\n<h3>1. Marketing Campaign Targeting</h3>\n<p>Target 78.7% of audience with just 6 campaigns instead of generic messaging. Each persona has evidence-based keywords and optimal timing data.</p>\n\n<h3>2. Pain Point Identification</h3>\n<p>23.2% of conversations reveal pain points: customer service chatbots (104 tweets), trust issues (175 tweets across 2 personas), and delivery tracking anxiety (98 tweets).</p>\n\n<h3>3. Emerging Trend Detection</h3>\n<p>The third-largest persona (13.3%) focuses on AI shopping and conversational commerce—signaling early adoption of AI-driven shopping experiences.</p>\n\n<h3>4. Market Intelligence</h3>\n<p>27% of dataset discusses India e-commerce, with specific insights on quick commerce (Blinkit, Zepto), competitor mentions, and logistics challenges.</p>\n\n<h2>Key Takeaways</h2>\n\n<ul>\n<li><strong>Feature fusion matters:</strong> 75% text embeddings + 25% behavioral features outperforms either alone</li>\n<li><strong>Noise handling enables 100% coverage:</strong> Nearest-centroid assignment preserves quality while using all data</li>\n<li><strong>Production-ready systems create value:</strong> Research without API integration has limited business impact</li>\n<li><strong>Automated discovery finds unexpected patterns:</strong> The Conversational Commerce persona emerged organically from the data</li>\n</ul>\n\n<h2>Code & Data</h2>\n<p>The complete system includes:</p>\n<ul>\n<li>~4,500 lines of production code</li>\n<li>18 API methods for querying and filtering</li>\n<li>Comprehensive documentation and examples</li>\n<li>Frozen configuration for reproducibility</li>\n</ul>\n\n<p><strong>System metrics:</strong> Coherence 0.80, ARI 0.75, 89.9% data retention, 100% coverage across 11 personas.</p>",
  "badge": "Architecture",
  "area": "Models & Embeddings",
  "difficulty": "Advanced",
  "readTime": "18 min",
  "hasCode": true,
  "hasVideo": false,
  "hasNotebook": false,
  "author": {
    "name": "RI Research Team",
    "role": "ML Engineering"
  },
  "views": 0,
  "usedByTeammates": 0
}
